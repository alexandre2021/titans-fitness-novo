# ü§ñ Central de Ajuda com IA (Planejamento)

Este documento descreve a arquitetura e a estrat√©gia para a cria√ß√£o de uma Central de Ajuda inteligente para o Titans Fitness.

---

## 1. Vis√£o Geral

O objetivo √© criar um sistema de suporte escal√°vel que combine uma base de conhecimento (Knowledge Base - KB) com um chatbot de Intelig√™ncia Artificial. A IA ser√° treinada para responder d√∫vidas dos usu√°rios com base exclusivamente no conte√∫do que n√≥s criarmos.

## 2. Componentes da Solu√ß√£o

### 2.1. Base de Conhecimento (Knowledge Base - KB)

-   **Fonte da Verdade:** O arquivo `src/docs/base-de-conhecimento.md` ser√° a √∫nica fonte de conte√∫do para a IA.
-   **Conte√∫do (V1):** Foco total em **texto**. N√£o incluiremos imagens ou GIFs nesta primeira vers√£o para agilizar o desenvolvimento e evitar retrabalho, j√° que a interface ainda pode mudar.

### 2.2. Chatbot com IA (RAG)

-   **Tecnologia:** A IA utilizar√° a t√©cnica de **Gera√ß√£o Aumentada por Recupera√ß√£o (RAG)**.
-   **Fluxo de Resposta:**
    1.  O usu√°rio faz uma pergunta no chat.
    2.  O sistema **busca (recupera)** na nossa Base de Conhecimento os artigos mais relevantes para a pergunta.
    3.  Uma LLM **gera** uma resposta em linguagem natural, utilizando *apenas* as informa√ß√µes encontradas nos artigos.
-   **Vantagem:** Isso garante que as respostas sejam sempre precisas e baseadas na nossa documenta√ß√£o, evitando que a IA "alucine" ou d√™ informa√ß√µes incorretas.

## 3. Ciclo de Melhoria Cont√≠nua (Human-in-the-loop)

### 2.3. Gera√ß√£o de Embeddings (Vetores)

-   **Ferramenta:** O script `scripts/generate-embeddings.ts` √© o respons√°vel por "ensinar" a IA.
-   **Processo:**
    1.  **Leitura:** O script l√™ o conte√∫do do arquivo `base-de-conhecimento.md`.
    2.  **Fragmenta√ß√£o (Chunking):** Ele quebra os artigos longos em peda√ßos menores e mais focados.
    3.  **Vetoriza√ß√£o:** Para cada peda√ßo, ele usa um modelo de IA local para gerar um "embedding" (vetor num√©rico).
    4.  **Armazenamento:** Salva cada peda√ßo de texto e seu respectivo vetor na tabela `knowledge_base` do Supabase.

> Este script deve ser executado sempre que a base de conhecimento for atualizada.

---

## 3. Ciclo de Melhoria Cont√≠nua (Human-in-the-loop)

Este √© o pilar para a evolu√ß√£o do sistema.

### 3.1. Tratamento de "Pontos Cegos"

Quando a IA n√£o encontra uma resposta na KB:

1.  **N√£o Inventa:** A IA n√£o tentar√° adivinhar.
2.  **Informa o Usu√°rio:** Ela responder√° que n√£o encontrou a informa√ß√£o, mas que a equipe de suporte j√° foi notificada e entrar√° em contato.
3.  **Notifica a Equipe:** O sistema automaticamente cria um "ticket" para n√≥s (registrando em uma tabela como `unanswered_questions`) e nos notifica ativamente.
    -   **Mecanismo de Notifica√ß√£o:** A Edge Function do chat invocar√° nossa pr√≥pria Edge Function `enviar-notificacao`, enviando uma mensagem para o chat do administrador (`contato@titans.fitness`).

### 3.2. Retroalimenta√ß√£o da Base de Conhecimento

1.  Ao receber a notifica√ß√£o, nossa equipe pesquisa a resposta correta.
2.  Respondemos ao usu√°rio manualmente (ex: por e-mail ou chat).
3.  **Crucial:** Usamos essa mesma resposta para **criar um novo artigo** na Base de Conhecimento.

**Resultado:** Da pr√≥xima vez que a mesma pergunta for feita, a IA saber√° a resposta. O sistema se torna mais inteligente a cada d√∫vida real do usu√°rio.

## 4. Prote√ß√£o Contra Abuso

-   Perguntas fora de t√≥pico (ex: "Qual o campe√£o do Brasileir√£o?") ser√£o identificadas pela baixa relev√¢ncia na busca da KB.
-   Nesses casos, o sistema dar√° uma resposta padr√£o ("S√≥ consigo responder sobre o Titans Fitness...") e **n√£o** criar√° um ticket para a equipe, evitando ru√≠do e perda de tempo.

---

## 5. Fase 2: Implementa√ß√£o da IA (Usu√°rios Logados)

### 5.1. Backend (Supabase Edge Function)

1.  **Receber a Pergunta:** A fun√ß√£o receber√° a pergunta do usu√°rio.
2.  **Buscar na Base de Conhecimento (RAG):**
    -   Converter√° a pergunta em um "vetor".
    -   Buscar√° por similaridade em uma base de vetores gerada a partir do `base-de-conhecimento.md`.
    -   Retornar√° os trechos de texto mais relevantes.
3.  **Gerar a Resposta (LLM):**
    -   Enviar√° os trechos para uma LLM com a instru√ß√£o: "Responda usando apenas este texto".
4.  **Devolver a Resposta:** Enviar√° a resposta gerada de volta para o chat.

### 5.2. Modelo de LLM Escolhido (V1)

-   **Provedor:** Groq (Plano Gratuito)
-   **Modelo:** `llama-3.1-8b-instant`
-   **Justificativa:**
    -   **Lat√™ncia:** Otimizado para respostas r√°pidas, ideal para chat.
    -   **Custo:** Limites generosos no plano gratuito (500.000 tokens/dia).
    -   **Capacidade:** Com base em uma estimativa de 300-900 tokens por intera√ß√£o, o plano gratuito suporta entre **600 a 1.600 respostas por dia**, o que √© mais do que suficiente para o lan√ßamento e valida√ß√£o.
    -   **Fidelidade:** √â um modelo do tipo `instruct`, treinado para seguir o contexto fornecido, o que √© perfeito para RAG.

### 5.3. Mecanismo de Aprendizado (Human-in-the-loop)

1.  **Detectar "Ponto Cego":** Se a busca n√£o encontrar trechos relevantes.
2.  **Registrar Pergunta:** Salvar√° a pergunta em uma nova tabela `perguntas_nao_respondidas`.
3.  **Notificar a Equipe:** Chamar√° a fun√ß√£o `enviar-notificacao` para nos avisar no chat de administrador.
4.  **Responder ao Usu√°rio:** A IA informar√° que a equipe foi notificada.

---

## 6. Status Atual e Desafios (Feature Pausada)

Apesar da arquitetura bem definida, a implementa√ß√£o pr√°tica do assistente de IA enfrentou desafios significativos que levaram √† decis√£o de pausar seu desenvolvimento. O objetivo desta se√ß√£o √© documentar o que foi constru√≠do e os obst√°culos encontrados.

### 6.1. O que foi Implementado

A arquitetura RAG foi totalmente implementada, incluindo:

-   **Frontend (`HelpChat.tsx`):** O componente React respons√°vel pela interface do chat. Ele gerencia o estado da conversa, captura a pergunta do usu√°rio e, crucialmente, utiliza a biblioteca `@xenova/transformers` para gerar o vetor (embedding) da pergunta diretamente no navegador do cliente. Em seguida, ele envia a pergunta e o embedding para a Edge Function.
-   **Edge Function (`ask-ai-help-center`):** Orquestra todo o fluxo: recebe a pergunta e o embedding, consulta o banco de vetores e chama a LLM.
-   **Busca Vetorial (Supabase `pgvector`):** A fun√ß√£o `match_knowledge_base` foi criada e utilizada para encontrar os trechos de texto mais relevantes.
-   **Gera√ß√£o de Resposta (Groq):** A integra√ß√£o com a API da Groq para o modelo `llama-3.1-8b-instant` foi conclu√≠da e funcionava conforme o esperado quando recebia o contexto correto.

### 6.2. O Ponto Fraco: Qualidade da Busca

O principal problema foi a **baixa efici√™ncia do assistente em encontrar as respostas corretas**, mesmo quando a informa√ß√£o existia claramente na base de conhecimento. A IA frequentemente respondia que "n√£o sabia" para perguntas b√°sicas.

A investiga√ß√£o revelou que a causa raiz n√£o era a LLM, mas sim a **qualidade da busca por similaridade**, que era impactada por dois fatores:

1.  **Ajuste de Limiar (`match_threshold`):** Tentamos ajustar o limiar de relev√¢ncia na busca vetorial. Embora tenha trazido melhorias marginais, n√£o resolveu o problema central, indicando que a quest√£o era mais profunda.

2.  **Qualidade dos Embeddings (A Causa Principal):** Identificamos que o problema real estava na forma como "ensin√°vamos" a IA. Nossos artigos na `base-de-conhecimento.md` eram longos e misturavam v√°rios assuntos. Isso gerava embeddings "generalistas", que n√£o eram espec√≠ficos o suficiente para corresponder com alta precis√£o a uma pergunta focada do usu√°rio.

### 6.3. A Tentativa de Solu√ß√£o: "Chunking"

Para resolver o problema da qualidade dos embeddings, implementamos a t√©cnica de **"chunking" (fragmenta√ß√£o)**. O script `scripts/generate-embeddings.ts` foi modificado para quebrar os artigos grandes em peda√ßos menores e mais focados, gerando um embedding para cada "chunk".

### 6.4. O Obst√°culo Final e a Decis√£o de Pausar

A implementa√ß√£o do "chunking" nos levou ao obst√°culo final: a complexidade de criar um "parser" robusto para o nosso arquivo `base-de-conhecimento.md`. O arquivo evoluiu e passou a ter m√∫ltiplos formatos de artigo, e a fun√ß√£o `parseKnowledgeBase` no script `generate-embeddings.ts` falhava consistentemente em ler todos os artigos, resultando em uma base de vetores vazia ou incompleta.

Devido √† dificuldade em garantir uma pipeline de dados confi√°vel (do `.md` para os vetores no Supabase) e o consequente mau desempenho do chatbot, **decidimos pausar o desenvolvimento desta funcionalidade**.

**Ponto de Partida Futuro:** A arquitetura principal (Frontend -> Edge Function -> LLM) est√° validada. O foco para retomar o projeto deve ser a cria√ß√£o de um sistema de gerenciamento de conte√∫do (CMS) ou um processo de ETL (Extra√ß√£o, Transforma√ß√£o e Carga) muito mais robusto e test√°vel para popular a base de conhecimento, garantindo a qualidade e a integridade dos vetores.

### 5.4. Interface do Chat (Frontend)

1.  **Localiza√ß√£o:** Um √≠cone de "?" flutuante no canto da tela para usu√°rios logados.
2.  **Funcionalidades:**
    -   Janela de conversa.
    -   Campo de texto para a pergunta.
    -   Indicador de "digitando...".
    -   A interface se comunicar√° com a nova Edge Function.